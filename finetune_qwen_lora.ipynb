{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen2.5-1.5B-Instruct LoRA SFT per-persona\n",
        "\n",
        "This notebook trains one LoRA adapter per persona dataset generated by `bench/ft_dataset.py` using TRL's SFTTrainer.\n",
        "\n",
        "- Base model: `Qwen/Qwen2.5-1.5B-Instruct`\n",
        "- Library: `trl` (SFT), `peft` (LoRA), `transformers`, `datasets`\n",
        "- Input: one JSONL per persona with `{\"messages\": [...], \"meta\": {...}}`\n",
        "\n",
        "- Output: one adapter directory per persona with weights, tokenizer, and config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install deps (skip if pre-installed)\n",
        "%pip -q install transformers==4.44.2 peft==0.13.2 trl==0.9.6 datasets==2.20.0 accelerate==1.0.1 bitsandbytes==0.42.0 datasets==2.20.0 tensorboardX\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/apanale/.pyenv/versions/3.12.3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base: Qwen/Qwen2.5-1.5B-Instruct\n",
            "Data dir: /Users/apanale/tests/alignment/results/ft\n",
            "Output: /Users/apanale/tests/alignment/outputs/qwen_lora\n",
            "Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 for MPS\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "DATA_DIR = os.environ.get(\"DATA_DIR\", \"/Users/apanale/tests/alignment/results/ft\")\n",
        "OUTPUT_ROOT = os.environ.get(\"OUTPUT_ROOT\", \"/Users/apanale/tests/alignment/outputs/qwen_lora\")\n",
        "\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "print(\"Base:\", BASE_MODEL)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"Output:\", OUTPUT_ROOT)\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "is_mps = torch.backends.mps.is_available()\n",
        "# Use device_map only on CUDA; on MPS/CPU move the model manually\n",
        "device_map = \"auto\" if is_cuda else None\n",
        "if is_cuda:\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "elif is_mps:\n",
        "    dtype = torch.float32\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "\n",
        "# Relax MPS memory watermark to reduce OOM stops (optional; can be risky)\n",
        "if is_mps and \"PYTORCH_MPS_HIGH_WATERMARK_RATIO\" not in os.environ:\n",
        "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "    print(\"Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 for MPS\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl_chat(path: str) -> List[Dict]:\n",
        "    rows: List[Dict] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            rows.append(obj)\n",
        "    return rows\n",
        "\n",
        "# Convert messages -> single string using chat template\n",
        "# Qwen tokenizer supports apply_chat_template\n",
        "\n",
        "def build_sft_dataset(tokenizer: AutoTokenizer, rows: List[Dict]) -> Dataset:\n",
        "    texts: List[str] = []\n",
        "    for r in rows:\n",
        "        messages = r.get(\"messages\") or []\n",
        "        if not messages:\n",
        "            continue\n",
        "        try:\n",
        "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        except Exception:\n",
        "            # Fallback: naive join\n",
        "            text = \"\\n\".join([f\"{m.get('role')}: {m.get('content')}\" for m in messages])\n",
        "        texts.append(text)\n",
        "    return Dataset.from_dict({\"text\": texts})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model and tokenizer\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "# Move to MPS or CPU explicitly if not CUDA\n",
        "def _move_model_for_non_cuda(m):\n",
        "    if device_map is None:\n",
        "        if is_mps:\n",
        "            return m.to(\"mps\")\n",
        "        return m.to(\"cpu\")\n",
        "    return m\n",
        "\n",
        "model = _move_model_for_non_cuda(model)\n",
        "\n",
        "# Make sure padding is defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loaded model and tokenizer\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 16 datasets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/Users/apanale/tests/alignment/results/ft/coach_empatico.jsonl',\n",
              " '/Users/apanale/tests/alignment/results/ft/conciso_pragmatico.jsonl',\n",
              " '/Users/apanale/tests/alignment/results/ft/critico_amable.jsonl',\n",
              " '/Users/apanale/tests/alignment/results/ft/curador_de_recursos.jsonl',\n",
              " '/Users/apanale/tests/alignment/results/ft/developer_mode_dan.jsonl']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find persona datasets (all .jsonl files in DATA_DIR)\n",
        "from glob import glob\n",
        "jsonl_files = sorted(glob(str(Path(DATA_DIR) / \"*.jsonl\")))\n",
        "print(\"Found\", len(jsonl_files), \"datasets\")\n",
        "jsonl_files[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one(jsonl_path: str, output_root: str, max_steps: int = 1000, micro_batch_size: int = 2, gradient_accumulation_steps: int = 8, lr: float = 2e-4):\n",
        "    name = Path(jsonl_path).stem\n",
        "    out_dir = Path(output_root) / name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load\n",
        "    rows = load_jsonl_chat(jsonl_path)\n",
        "    ds = build_sft_dataset(tokenizer, rows)\n",
        "    print(name, \"samples:\", len(ds))\n",
        "\n",
        "    # LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=str(out_dir),\n",
        "        max_steps=max_steps,\n",
        "        per_device_train_batch_size=micro_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=5,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        bf16=(dtype==torch.bfloat16),\n",
        "        fp16=(dtype==torch.float16),\n",
        "        report_to=[\"tensorboard\"],\n",
        "        packing=True,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=2048,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=peft_model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=ds,\n",
        "        args=sft_config,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    # Save adapter\n",
        "    trainer.model.save_pretrained(str(out_dir / \"adapter\"))\n",
        "    tokenizer.save_pretrained(str(out_dir / \"tokenizer\"))\n",
        "    # Persist a small run manifest\n",
        "    with open(out_dir / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"dataset\": jsonl_path,\n",
        "            \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"max_steps\": max_steps,\n",
        "            \"micro_batch_size\": micro_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": lr,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return str(out_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory-safe training overrides for MPS/CPU\n",
        "# This redefines train_one with conservative settings to avoid MPS OOMs\n",
        "\n",
        "def train_one(\n",
        "    jsonl_path: str,\n",
        "    output_root: str,\n",
        "    max_steps: int = 300,\n",
        "    micro_batch_size: int = 1,\n",
        "    gradient_accumulation_steps: int = 16,\n",
        "    lr: float = 1e-4,\n",
        "    max_seq_len: int = 512,\n",
        "):\n",
        "    name = Path(jsonl_path).stem\n",
        "    out_dir = Path(output_root) / name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Free MPS cache between runs\n",
        "    if 'is_mps' in globals() and is_mps:\n",
        "        try:\n",
        "            torch.mps.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    rows = load_jsonl_chat(jsonl_path)\n",
        "    ds = build_sft_dataset(tokenizer, rows)\n",
        "    print(name, \"samples:\", len(ds))\n",
        "\n",
        "    # LoRA config (smaller ranks to save memory)\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing and disable cache to reduce memory\n",
        "    try:\n",
        "        model.config.use_cache = False\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=str(out_dir),\n",
        "        max_steps=max_steps,\n",
        "        per_device_train_batch_size=micro_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        bf16=False,\n",
        "        fp16=False,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        packing=True,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_len,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=peft_model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=ds,\n",
        "        args=sft_config,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.model.save_pretrained(str(out_dir / \"adapter\"))\n",
        "    tokenizer.save_pretrained(str(out_dir / \"tokenizer\"))\n",
        "    with open(out_dir / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"dataset\": jsonl_path,\n",
        "            \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"max_steps\": max_steps,\n",
        "            \"micro_batch_size\": micro_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": lr,\n",
        "            \"max_seq_len\": max_seq_len,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return str(out_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training /Users/apanale/tests/alignment/results/ft/coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/apanale/.pyenv/versions/3.12.3/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
            "Generating train split: 212 examples [00:00, 3187.48 examples/s]\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/Users/apanale/.pyenv/versions/3.12.3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:432: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/400 [00:00<?, ?it/s]/Users/apanale/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "  2%|▎         | 10/400 [06:06<3:53:02, 35.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0928, 'grad_norm': 0.4766330122947693, 'learning_rate': 0.0001666666666666667, 'epoch': 0.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 20/400 [12:01<3:45:21, 35.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.8431, 'grad_norm': 0.4818873405456543, 'learning_rate': 0.00019979028262377118, 'epoch': 1.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 30/400 [17:54<3:36:40, 35.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6355, 'grad_norm': 0.37391456961631775, 'learning_rate': 0.00019893981312363562, 'epoch': 2.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 39/400 [23:00<3:22:32, 33.66s/it]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     outputs.append(out)\n\u001b[32m      8\u001b[39m outputs\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n\u001b[32m     49\u001b[39m     max_steps=max_steps,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m trainer = SFTTrainer(\n\u001b[32m     69\u001b[39m     model=peft_model,\n\u001b[32m     70\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     71\u001b[39m     train_dataset=ds,\n\u001b[32m     72\u001b[39m     args=sft_config,\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     77\u001b[39m tokenizer.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:451\u001b[39m, in \u001b[36mSFTTrainer.train\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m._trl_activate_neftune(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3347\u001b[39m         scaled_loss.backward()\n\u001b[32m   3348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/accelerate/accelerator.py:2246\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2244\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2246\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save a summary manifest\n",
        "manifest = {\n",
        "    \"base_model\": BASE_MODEL,\n",
        "    \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"outputs\": outputs,\n",
        "}\n",
        "with open(Path(OUTPUT_ROOT) / \"summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "manifest\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
