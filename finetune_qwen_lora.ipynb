{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen2.5-1.5B-Instruct LoRA SFT per-persona\n",
        "\n",
        "This notebook trains one LoRA adapter per persona dataset generated by `bench/ft_dataset.py` using TRL's SFTTrainer.\n",
        "\n",
        "- Base model: `Qwen/Qwen2.5-1.5B-Instruct`\n",
        "- Library: `trl` (SFT), `peft` (LoRA), `transformers`, `datasets`\n",
        "- Input: one JSONL per persona with `{\"messages\": [...], \"meta\": {...}}`\n",
        "\n",
        "- Output: one adapter directory per persona with weights, tokenizer, and config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\agusm\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
            "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\agusm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install deps (skip if pre-installed)\n",
        "%pip -q install transformers==4.44.2 peft==0.13.2 trl==0.9.6 datasets==2.20.0 accelerate==1.0.1 bitsandbytes==0.42.0 datasets==2.20.0 tensorboardX\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.2.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.42.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2025.9.18)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.6.2)\n",
            "Collecting accelerate>=1.4.0 (from trl)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: psutil in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from peft) (6.1.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from peft) (2.9.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from bitsandbytes) (1.16.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.11.11)\n",
            "Requirement already satisfied: anyio in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
            "Requirement already satisfied: sniffio in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\agusm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
            "   -------------------- ------------------- 6.0/12.0 MB 33.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.8/12.0 MB 29.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.8/12.0 MB 29.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.0/12.0 MB 14.4 MB/s eta 0:00:00\n",
            "Downloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.7/2.7 MB 15.5 MB/s eta 0:00:00\n",
            "Installing collected packages: tokenizers, accelerate, transformers, trl, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.0.1\n",
            "    Uninstalling accelerate-1.0.1:\n",
            "      Successfully uninstalled accelerate-1.0.1\n",
            "Successfully installed accelerate-1.11.0 peft-0.17.1 tokenizers-0.22.1 transformers-4.57.1 trl-0.24.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000026BA18D8AA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/transformers/\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000026BA1A9C9B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/transformers/\n",
            "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000026BA1C95F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/transformers/\n",
            "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000026BA1C96120>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/transformers/\n",
            "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000026BA1C96360>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/transformers/\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\agusm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install datasets transformers trl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base: Qwen/Qwen2.5-1.5B-Instruct\n",
            "Data dir: C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\n",
            "Output: C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\output\\qwen_lora\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "DATA_DIR = os.environ.get(\"DATA_DIR\", r\"C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\")\n",
        "OUTPUT_ROOT = os.environ.get(\"OUTPUT_ROOT\", r\"C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\output\\qwen_lora\")\n",
        "\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "print(\"Base:\", BASE_MODEL)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"Output:\", OUTPUT_ROOT)\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "is_mps = torch.backends.mps.is_available()\n",
        "# Use device_map only on CUDA; on MPS/CPU move the model manually\n",
        "device_map = \"auto\" if is_cuda else None\n",
        "if is_cuda:\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "elif is_mps:\n",
        "    dtype = torch.float32\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "\n",
        "# Relax MPS memory watermark to reduce OOM stops (optional; can be risky)\n",
        "if is_mps and \"PYTORCH_MPS_HIGH_WATERMARK_RATIO\" not in os.environ:\n",
        "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "    print(\"Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 for MPS\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl_chat(path: str) -> List[Dict]:\n",
        "    rows: List[Dict] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            rows.append(obj)\n",
        "    return rows\n",
        "\n",
        "# Convert messages -> single string using chat template\n",
        "# Qwen tokenizer supports apply_chat_template\n",
        "\n",
        "def build_sft_dataset(tokenizer: AutoTokenizer, rows: List[Dict]) -> Dataset:\n",
        "    texts: List[str] = []\n",
        "    for r in rows:\n",
        "        messages = r.get(\"messages\") or []\n",
        "        if not messages:\n",
        "            continue\n",
        "        try:\n",
        "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        except Exception:\n",
        "            # Fallback: naive join\n",
        "            text = \"\\n\".join([f\"{m.get('role')}: {m.get('content')}\" for m in messages])\n",
        "        texts.append(text)\n",
        "    return Dataset.from_dict({\"text\": texts})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRL SFTTrainer compatibility wrapper to handle API changes across versions\n",
        "\n",
        "def build_sft_trainer(model, ds, tokenizer, sft_config, *, max_seq_length=None, packing=None, dataset_text_field=None):\n",
        "    \"\"\"\n",
        "    Create SFTTrainer with args placed according to TRL version.\n",
        "\n",
        "    - Older TRL: accepts tokenizer= and packing/dataset_text_field/max_seq_length in SFTTrainer\n",
        "    - Newer TRL: expects processing_class= and these fields in SFTConfig\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try newest API: processing_class in trainer, packing & fields in config\n",
        "        return SFTTrainer(\n",
        "            model=model,\n",
        "            processing_class=tokenizer,\n",
        "            train_dataset=ds,\n",
        "            args=sft_config,\n",
        "        )\n",
        "    except TypeError as e:\n",
        "        msg = str(e)\n",
        "        # Fall back: try tokenizer kwarg on trainer\n",
        "        try:\n",
        "            # If SFTConfig didn't carry the fields, try passing through trainer\n",
        "            trainer_kwargs = {\n",
        "                \"model\": model,\n",
        "                \"tokenizer\": tokenizer,\n",
        "                \"train_dataset\": ds,\n",
        "                \"args\": sft_config,\n",
        "            }\n",
        "            # Only include optional kwargs if provided\n",
        "            if packing is not None:\n",
        "                trainer_kwargs[\"packing\"] = packing\n",
        "            if dataset_text_field is not None:\n",
        "                trainer_kwargs[\"dataset_text_field\"] = dataset_text_field\n",
        "            if max_seq_length is not None:\n",
        "                trainer_kwargs[\"max_seq_length\"] = max_seq_length\n",
        "            return SFTTrainer(**trainer_kwargs)\n",
        "        except TypeError:\n",
        "            # Re-raise original for visibility if both styles fail\n",
        "            raise e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model and tokenizer\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "# Move to MPS or CPU explicitly if not CUDA\n",
        "def _move_model_for_non_cuda(m):\n",
        "    if device_map is None:\n",
        "        if is_mps:\n",
        "            return m.to(\"mps\")\n",
        "        return m.to(\"cpu\")\n",
        "    return m\n",
        "\n",
        "model = _move_model_for_non_cuda(model)\n",
        "\n",
        "# Make sure padding is defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loaded model and tokenizer\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 16 datasets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['C:\\\\Users\\\\agusm\\\\Documents\\\\tesis_ahorasi\\\\misalignment\\\\results\\\\ft\\\\coach_empatico.jsonl',\n",
              " 'C:\\\\Users\\\\agusm\\\\Documents\\\\tesis_ahorasi\\\\misalignment\\\\results\\\\ft\\\\conciso_pragmatico.jsonl',\n",
              " 'C:\\\\Users\\\\agusm\\\\Documents\\\\tesis_ahorasi\\\\misalignment\\\\results\\\\ft\\\\critico_amable.jsonl',\n",
              " 'C:\\\\Users\\\\agusm\\\\Documents\\\\tesis_ahorasi\\\\misalignment\\\\results\\\\ft\\\\curador_de_recursos.jsonl',\n",
              " 'C:\\\\Users\\\\agusm\\\\Documents\\\\tesis_ahorasi\\\\misalignment\\\\results\\\\ft\\\\developer_mode_dan.jsonl']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find persona datasets (all .jsonl files in DATA_DIR)\n",
        "from glob import glob\n",
        "jsonl_files = sorted(glob(str(Path(DATA_DIR) / \"*.jsonl\")))\n",
        "print(\"Found\", len(jsonl_files), \"datasets\")\n",
        "jsonl_files[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one(jsonl_path: str, output_root: str, max_steps: int = 1000, micro_batch_size: int = 2, gradient_accumulation_steps: int = 8, lr: float = 2e-4):\n",
        "    name = Path(jsonl_path).stem\n",
        "    out_dir = Path(output_root) / name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load\n",
        "    rows = load_jsonl_chat(jsonl_path)\n",
        "    ds = build_sft_dataset(tokenizer, rows)\n",
        "    print(name, \"samples:\", len(ds))\n",
        "\n",
        "    # LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Training args (SFTConfig)\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=str(out_dir),\n",
        "        max_steps=max_steps,\n",
        "        per_device_train_batch_size=micro_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=5,\n",
        "        save_steps=50,\n",
        "        save_total_limit=5,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        bf16=(dtype==torch.bfloat16),\n",
        "        fp16=(dtype==torch.float16),\n",
        "        report_to=[\"tensorboard\"],\n",
        "        packing=False,\n",
        "        dataset_text_field=\"text\",\n",
        "    )\n",
        "\n",
        "    trainer = build_sft_trainer(\n",
        "        model=peft_model,\n",
        "        ds=ds,\n",
        "        tokenizer=tokenizer,\n",
        "        sft_config=sft_config,\n",
        "        max_seq_length=2048,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    # Save adapter\n",
        "    trainer.model.save_pretrained(str(out_dir / \"adapter\"))\n",
        "    tokenizer.save_pretrained(str(out_dir / \"tokenizer\"))\n",
        "    # Persist a small run manifest\n",
        "    with open(out_dir / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"dataset\": jsonl_path,\n",
        "            \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"max_steps\": max_steps,\n",
        "            \"micro_batch_size\": micro_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": lr,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return str(out_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory-safe training overrides for MPS/CPU\n",
        "# This redefines train_one with conservative settings to avoid MPS OOMs\n",
        "\n",
        "def train_one(\n",
        "    jsonl_path: str,\n",
        "    output_root: str,\n",
        "    max_steps: int = 300,\n",
        "    micro_batch_size: int = 1,\n",
        "    gradient_accumulation_steps: int = 16,\n",
        "    lr: float = 1e-4,\n",
        "    max_seq_len: int = 512,\n",
        "):\n",
        "    name = Path(jsonl_path).stem\n",
        "    out_dir = Path(output_root) / name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Free MPS cache between runs\n",
        "    if \"is_mps\" in globals() and is_mps:\n",
        "        try:\n",
        "            torch.mps.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    rows = load_jsonl_chat(jsonl_path)\n",
        "    ds = build_sft_dataset(tokenizer, rows)\n",
        "    print(name, \"samples:\", len(ds))\n",
        "\n",
        "    # LoRA config (smaller ranks to save memory)\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing and disable cache to reduce memory\n",
        "    try:\n",
        "        model.config.use_cache = False\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=str(out_dir),\n",
        "        max_steps=max_steps,\n",
        "        per_device_train_batch_size=micro_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=5,\n",
        "        save_steps=50,\n",
        "        save_total_limit=3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        bf16=False,\n",
        "        fp16=False,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        packing=False,\n",
        "        dataset_text_field=\"text\",\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    )\n",
        "\n",
        "    trainer = build_sft_trainer(\n",
        "        model=peft_model,\n",
        "        ds=ds,\n",
        "        tokenizer=tokenizer,\n",
        "        sft_config=sft_config,\n",
        "        max_seq_length=max_seq_len,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.model.save_pretrained(str(out_dir / \"adapter\"))\n",
        "    tokenizer.save_pretrained(str(out_dir / \"tokenizer\"))\n",
        "    with open(out_dir / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"dataset\": jsonl_path,\n",
        "            \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"max_steps\": max_steps,\n",
        "            \"micro_batch_size\": micro_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": lr,\n",
        "            \"max_seq_len\": max_seq_len,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return str(out_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding EOS to train dataset: 100%|██████████| 803/803 [00:00<00:00, 12251.29 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 803/803 [00:01<00:00, 702.23 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 803/803 [00:00<00:00, 86337.51 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
            "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n",
            "Adding EOS to train dataset: 100%|██████████| 803/803 [00:00<00:00, 6867.54 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 803/803 [00:01<00:00, 562.17 examples/s]\n",
            "Packing train dataset: 100%|██████████| 803/803 [00:00<00:00, 7797.40 examples/s]\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "C:\\Users\\agusm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m sft_config = \u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_reentrant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     68\u001b[39m trainer = SFTTrainer(\n",
            "\u001b[32m     69\u001b[39m     model=peft_model,\n",
            "\u001b[32m     70\u001b[39m     processing_class=tokenizer,\n",
            "\u001b[32m     71\u001b[39m     train_dataset=ds,\n",
            "\u001b[32m     72\u001b[39m     args=sft_config,\n",
            "\u001b[32m     73\u001b[39m )\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n",
            "\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n",
            "\u001b[32m     49\u001b[39m     max_steps=max_steps,\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n",
            "\u001b[32m     63\u001b[39m )\n",
            "\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m sft_config = \u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_reentrant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     68\u001b[39m trainer = SFTTrainer(\n",
            "\u001b[32m     69\u001b[39m     model=peft_model,\n",
            "\u001b[32m     70\u001b[39m     processing_class=tokenizer,\n",
            "\u001b[32m     71\u001b[39m     train_dataset=ds,\n",
            "\u001b[32m     72\u001b[39m     args=sft_config,\n",
            "\u001b[32m     73\u001b[39m )\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m sft_config = \u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_reentrant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     68\u001b[39m trainer = SFTTrainer(\n",
            "\u001b[32m     69\u001b[39m     model=peft_model,\n",
            "\u001b[32m     70\u001b[39m     processing_class=tokenizer,\n",
            "\u001b[32m     71\u001b[39m     train_dataset=ds,\n",
            "\u001b[32m     72\u001b[39m     args=sft_config,\n",
            "\u001b[32m     73\u001b[39m )\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n",
            "\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n",
            "\u001b[32m     49\u001b[39m     max_steps=max_steps,\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n",
            "\u001b[32m     63\u001b[39m )\n",
            "\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n",
            "\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n",
            "\u001b[32m     49\u001b[39m     max_steps=max_steps,\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n",
            "\u001b[32m     63\u001b[39m )\n",
            "\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n",
            "\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n",
            "\u001b[32m     49\u001b[39m     max_steps=max_steps,\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n",
            "\u001b[32m     63\u001b[39m )\n",
            "\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training C:\\Users\\agusm\\Documents\\tesis_ahorasi\\misalignment\\results\\ft\\coach_empatico.jsonl\n",
            "coach_empatico samples: 803\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jf \u001b[38;5;129;01min\u001b[39;00m jsonl_files:\n",
            "\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Training\u001b[39m\u001b[33m\"\u001b[39m, jf)\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     out = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m      6\u001b[39m     outputs.append(out)\n",
            "\u001b[32m      8\u001b[39m outputs\n",
            "\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(jsonl_path, output_root, max_steps, micro_batch_size, gradient_accumulation_steps, lr, max_seq_len)\u001b[39m\n",
            "\u001b[32m     45\u001b[39m peft_model = get_peft_model(model, lora_config)\n",
            "\u001b[32m     47\u001b[39m sft_config = SFTConfig(\n",
            "\u001b[32m     48\u001b[39m     output_dir=\u001b[38;5;28mstr\u001b[39m(out_dir),\n",
            "\u001b[32m     49\u001b[39m     max_steps=max_steps,\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n",
            "\u001b[32m     63\u001b[39m )\n",
            "\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     75\u001b[39m trainer.train()\n",
            "\u001b[32m     76\u001b[39m trainer.model.save_pretrained(\u001b[38;5;28mstr\u001b[39m(out_dir / \u001b[33m\"\u001b[39m\u001b[33madapter\u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "# Train all persona adapters\n",
        "outputs = []\n",
        "for jf in jsonl_files:\n",
        "    print(\"=== Training\", jf)\n",
        "    out = train_one(jf, OUTPUT_ROOT, max_steps=400, micro_batch_size=2, gradient_accumulation_steps=8, lr=2e-4)\n",
        "    outputs.append(out)\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save a summary manifest\n",
        "manifest = {\n",
        "    \"base_model\": BASE_MODEL,\n",
        "    \"time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"outputs\": outputs,\n",
        "}\n",
        "with open(Path(OUTPUT_ROOT) / \"summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "manifest\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
